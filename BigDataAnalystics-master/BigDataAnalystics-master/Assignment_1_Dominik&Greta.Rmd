---
title: "Assignment_1_Greta&Dominik"
author: "Dominik Walter"
date: "22 4 2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readtext)
library(quanteda)
library(quanteda.textmodels)
library(cowplot)
library(quanteda.textplots)
library(PerformanceAnalytics)
library(psych)
library(tidyverse)

```


#########################################################################
#########################################################################
# Creating the Corpus of the UK party manifestos since 2010
#########################################################################
#########################################################################
```{r}
myText <- readtext("UK new manifestoes/*.txt", 
                   docvarsfrom = "filenames", dvsep = " ", docvarnames = c("Party", "Year"))
myText <- gsub("[\u00E2]","",myText$text) 


testCorpus <- corpus(myText )
summary(testCorpus)
# I rename the name of the documents
docnames(testCorpus) <- gsub(".txt", "", docnames(testCorpus ))

summary(testCorpus)
```

#####################
# Remember! Comparing the results with and w/o stopwords, with and w/o stemming is always a good practice
#####################
```{r}
myDfm <- dfm(testCorpus , remove = stopwords("smart"), tolower = TRUE, stem = TRUE,
             remove_punct = TRUE, remove_numbers=TRUE, split_hyphens = TRUE)
topfeatures(myDfm , 20)  # 20 top words
myDfm <- dfm_remove(myDfm, min_nchar=2)
```
#########################################################################
#########################################################################
# Using Wordfish 
#########################################################################
#########################################################################
```{r}

# dir indicates which two documents are used for global identification purposes 
# (the first document to the left of the second one); 
# this matters usually more for the interpretatio of the results (i.e., for the direction of the scores 
# along the latent dimension (which positive, which negative ones)), rather than for the estimation per-se

# here: LAB 92 to the left of CONS 92
summary(testCorpus)
wfm <- textmodel_wordfish(myDfm, dir = c(4, 11))
summary(wfm)

```
# Using Wordscores
#########################################################################
#########################################################################

#########################################################################
#########################################################################
# Drop unique words and check correlation across documents 
#########################################################################
#########################################################################

```{r}
summary(testCorpus)

# keep only words occurring >1 times
myDfm <- dfm_trim(myDfm, min_termfreq = 2)

# compute some document similarities
Simil <- textstat_simil(myDfm , method = "correlation")
Simil

```


#########################################################################
#########################################################################
# Using wordscores: UK example with economic policy positions 
#########################################################################
#########################################################################

# reference texts:
# reference texts scores:
# reference scores derived from an expert survey

###############
# FIRST step:
# Set reference scores 
###############

```{r}
refscores_economy <- c(NA, 7.85, NA, NA, 3.85, NA, NA, 5.14, NA, NA, 8.57, NA)
refscores_eu <- c(NA,3.14,NA,NA,5.57,NA,NA,6.71,NA,NA,1.14,NA)
```


###############
# SECOND step: 
# Assign the reference scores to your dfm
###############
```{r}
ws_economy <- textmodel_wordscores(myDfm, refscores_economy)
ws_eu <- textmodel_wordscores(myDfm, refscores_eu)
summary(ws_economy)
summary(ws_eu) 
# Plot estimated word positions in the reference texts. It shows the frequency vs. the word-score


```


###############
# THIRD step: we predict the raw Wordscores for all the texts (reference and virgin ones)
###############
```{r}



pr_economy <- predict(ws_economy, rescaling = "lbg", newdata = myDfm[c(1,3,4,6,7,9,10,12), ])
pr_eu <- predict(ws_eu, rescaling = "lbg", newdata = myDfm[c(1,3,4,6,7,9,10,12), ])

pr_economy

pr_eu



# add the prediction scores to the reference scores --> DO ISCH MIS PROBLEM VILICHT WEISCH DU MEH@ GRETA
scores_economy <- c(5.690635, 5.825176, 4.927647, 4.506773, 5.195472, 4.940397, 9.907417, 9.905901)
scores_eu <- c(4.80248034, 4.76073830, 5.47054701, 5.52623858, 5.77116977, 6.43145194, 0.37347041, 0.02879203 )


# assign it to the texts
ws_economy <- textmodel_wordscores(myDfm,scores_economy)
ws_eu <-textmodel_wordscores(myDfm,scores_eu)
```



